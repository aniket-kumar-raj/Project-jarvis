<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Ultra Lightweight Palm Detector</title>
<style>
  body {
    margin: 0;
    background: #000;
    display: flex;
    justify-content: center;
    align-items: center;
    height: 100vh;
    overflow: hidden;
    font-family: Arial, sans-serif;
    position: relative;
  }

  #display {
    max-width: 100%;
    max-height: 100%;
    opacity: 0;
    transition: opacity 0.3s linear;
    border-radius: 10px;
  }

  #overlay {
    position: absolute;
    top:0; left:0; width:100%; height:100%;
    display:flex; justify-content:center; align-items:center;
    color:white; font-size:18px; text-align:center;
    background:rgba(0,0,0,0.85);
    opacity:1;
    transition: opacity 0.3s linear;
    padding:10px;
  }

  video { display:none; }
</style>
</head>
<body>

<video id="camera" autoplay playsinline width="80" height="60"></video>
<img id="display" src="">
<div id="overlay">Please show your palm</div>

<!-- Correct CDN -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/hand-pose-detection"></script>

<script>
const video = document.createElement('video');
video.width = 320;
video.height = 240;
video.autoplay = true;
document.body.appendChild(video);

async function start() {
  await navigator.mediaDevices.getUserMedia({ video: true }).then(stream => video.srcObject = stream);

  const model = handPoseDetection.SupportedModels.MediaPipeHands;
  const detector = await handPoseDetection.createDetector(model, {
    runtime: 'tfjs', // runs on CPU
    maxHands: 1,
  });

  async function detect() {
    const hands = await detector.estimateHands(video);
    if(hands.length > 0){
      console.log("Palm detected!", hands);
      // Show your image here
    }
    requestAnimationFrame(detect);
  }
  detect();
}
start();
</script>

</body>
</html>
